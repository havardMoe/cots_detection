{"cells":[{"cell_type":"markdown","metadata":{"id":"bEHmtIJ1z5fV"},"source":["# Installing WandB & Downgrading OpenCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lANdkSjl0DKT"},"outputs":[],"source":["%%capture\n","!pip install --upgrade pip\n","!pip install wandb\n","\n","# Downgrade opencv to solve some compatibility issues\n","!pip uninstall opencv-python-headless -y\n","!pip install opencv-python-headless==4.1.2.30"]},{"cell_type":"markdown","metadata":{"id":"mTCQa-vs0TMS"},"source":["# Log Into WandB, and Mount to Drive\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WU4ROAWl0RpQ"},"outputs":[],"source":["import tensorflow as tf\n","import tensorboard\n","from collections import defaultdict\n","from google.colab import drive\n","import tarfile\n","import re\n","import os\n","import wandb\n","import shutil\n","import csv\n","from tensorflow.python.summary.summary_iterator import summary_iterator\n","\n","drive.mount('/content/drive')\n","\n","!wandb login\n","\n","%load_ext tensorboard"]},{"cell_type":"markdown","metadata":{"id":"DqF2_FGyc7PO"},"source":["Install Tensorflow Object Detection API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJx3RSDz0LGY"},"outputs":[],"source":["%%bash\n","# Install the Tensorflow object detection API\n","# This installation may take a while\n","sudo apt -y install protobuf-compiler\n","cd /content/drive/MyDrive/models/research\n","protoc object_detection/protos/*.proto --python_out=.   \n","cp object_detection/packages/tf2/setup.py .\n","python -m pip install .\n","\n","# Test installataion\n","# python /content/drive/MyDrive/models/research/object_detection/builders/model_builder_tf2_test.py"]},{"cell_type":"markdown","metadata":{"id":"IM9PwdNmZ1wW"},"source":["# Initialization of Path and Globals"]},{"cell_type":"markdown","metadata":{"id":"kumoLZSLpAFO"},"source":["* Number of classes (always 1 for this project: 'cots')\n","* Choose how many prediction images to make for the tfevent-eval file\n","* Choose if you want to save the tfevent files in wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRd8K4Gbo88D"},"outputs":[],"source":["# Always 1 for this project (class: cots)\n","NUM_CLASSES = 1  \n","# Number of prediction images stored in tfevent-eval\n","NUM_VISUALIZATIONS = 5\n","# Storing tfevent files in the wandb-run\n","STORE_TFEVENT_FILES = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ft_toUBFZ4ZM"},"outputs":[],"source":["PRETRAINED_DIR = '/content/drive/MyDrive/models/research/deploy/pretrained/'\n","TRAIN_RECORD_FILE = '/content/drive/MyDrive/models/research/object_detection/train_cots/training_data/train.tfrecord'\n","VAL_RECORD_FILE = '/content/drive/MyDrive/models/research/object_detection/train_cots/training_data/validation.tfrecord'\n","TEST_RECORD_FILE = '/content/drive/MyDrive/models/research/object_detection/train_cots/training_data/test.tfrecord'\n","LABELMAP_FILE = '/content/drive/MyDrive/models/research/object_detection/train_cots/training_data/label_map.txt'\n","\n","MODELS_CONFIG = {\n","    'efficientdet-d0': {\n","        'model_name': 'efficientdet_d0_coco17_tpu-32',\n","        'base_config_path': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n","    },\n","    'resnet-50-faster': {\n","        'model_name': 'faster_rcnn_resnet50_v1_640x640_coco17_tpu-8',\n","        'base_config_path': 'faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz',\n","    },\n","    'resnet-50': {\n","        'model_name': 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8',\n","        'base_config_path': 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz'\n","    },\n","    'centernet': {\n","        'model_name': 'centernet_hg104_512x512_coco17_tpu-8',\n","        'base_config_path': '/content/drive/MyDrive/models/research/deploy/pretrained/centernet_hg104_512x512_coco17_tpu-8/pipeline.config',\n","        'pretrained_checkpoint': 'centernet_hg104_512x512_coco17_tpu-8.tar.gz'\n","    },\n","    'mobilenet': {\n","        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n","        'base_config_path': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n","        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz'\n","    }\n","}"]},{"cell_type":"markdown","metadata":{"id":"lT34dICr34AL"},"source":["# Train Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSlbPliM36_c"},"outputs":[],"source":["def train_eval(config, run_id):\n","    # initialization of model specific paths\n","    model = config['model']\n","    # dir for log training and metrics, gets reset every sweep\n","    model_dir = '/content/training/' + MODELS_CONFIG[model]['model_name']\n","    log_dir = '/content/drive/MyDrive/models/research/deploy/logs/' + run_id\n","    base_pipeline_file = MODELS_CONFIG[model]['base_config_path']\n","    pretrained_checkpoint = MODELS_CONFIG[model]['pretrained_checkpoint']\n","    fine_tune_checkpoint = PRETRAINED_DIR + MODELS_CONFIG[model]['model_name'] + '/checkpoint/ckpt-0'\n","    config_file = model_dir + '/deployed_config.config'\n","\n","    # reset if exists log_dir and model_dir\n","    if os.path.exists(log_dir):\n","        shutil.rmtree(log_dir)\n","    os.makedirs(log_dir, exist_ok=True)\n","\n","    if os.path.exists(model_dir):\n","        shutil.rmtree(model_dir)\n","    os.makedirs(model_dir, exist_ok=True)\n","\n","    # prepating environment\n","    prepare_env(model_dir, pretrained_checkpoint, base_pipeline_file)\n","\n","    # update config file to match sweep_config\n","    update_config_file(config, base_pipeline_file, config_file, fine_tune_checkpoint)\n","\n","    # Train the model, output get stored in model_dir\n","    !python /content/drive/MyDrive/models/research/object_detection/model_main_tf2.py \\\n","        --pipeline_config_path {config_file} \\\n","        --model_dir {model_dir} \\\n","        --sample_1_of_n_eval_examples=1 \\\n","        --checkpoint_every_n=1000 \\\n","        --record_summaries=False \\\n","        --alsologtostderr\n","    \n","\n","    # Test the model\n","    !python /content/drive/MyDrive/models/research/object_detection/model_main_tf2.py \\\n","        --pipeline_config_path {config_file} \\\n","        --model_dir {model_dir} \\\n","        --checkpoint_dir {model_dir} \\\n","        --eval_timeout=4 \\\n","        --wait_interval=2 \\\n","        --alsologtostderr\n","\n","\n","    # Move tfevent files from model_dir to log_dir\n","    !cp -r {model_dir + '/train'} {log_dir + '/train'}\n","    !cp -r {model_dir + '/eval'} {log_dir + '/eval'}\n","\n","    # Also store hpams in log_dir\n","    filename = 'hpams.csv'\n","    path = os.path.join(log_dir, filename)\n","    headers = list(config.keys())\n","    with open(path, 'w') as csvfile:\n","        writer = csv.DictWriter(csvfile, fieldnames=headers)\n","        writer.writeheader()\n","        writer.writerow(config)\n","    \n","\n","def prepare_env(model_dir, pretrained_checkpoint, base_pipeline_file):\n","    '''download config file, and relevant model \n","    information, also makes sure folders exist'''\n","\n","    # Download pretrained weights if they arent already\n","    %cd {PRETRAINED_DIR}\n","\n","    # check if tarfile file already exists\n","    if not os.path.exists(PRETRAINED_DIR + pretrained_checkpoint):\n","        download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n","        !wget {download_tar}\n","\n","    # check if unzipped tarfile folder exists\n","    if not os.path.exists(pretrained_checkpoint.split('.')[0]):\n","        tar = tarfile.open(PRETRAINED_DIR + pretrained_checkpoint)\n","        tar.extractall()\n","        tar.close()\n","\n","    # get config file and set path of config file\n","    if not os.path.exists(base_pipeline_file):\n","        download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n","        !wget {download_config}\n","\n","\n","def update_config_file(sweep_config, base_pipeline_file, config_file, fine_tune_checkpoint):\n","    '''\n","    * sweep_config (dict) w/keys:\n","        'batch_size'\n","        'num_steps'\n","        ... more to come\n","    * base_pipeline_file: path to template config for this model\n","    * config_file: where you want to store the update config file\n","    '''\n","\n","    with open(base_pipeline_file) as f:\n","        config = f.read()\n","\n","    if os.path.exists(config_file):\n","        os.remove(config_file)\n","        print('deleted old initialization cofig file')\n","        \n","    with open(config_file, 'w') as f:\n","        # Set labelmap path\n","        config = re.sub('label_map_path: \".*?\"', \n","                        'label_map_path: \"{}\"'.format(LABELMAP_FILE), config)\n","\n","        # Set fine_tune_checkpoint path\n","        config = re.sub('fine_tune_checkpoint: \".*?\"',\n","                        'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), config)\n","\n","        # Set train tf-record file path\n","        config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', \n","                        'input_path: \"{}\"'.format(TRAIN_RECORD_FILE), config)\n","\n","        # Set validation tf-record file path\n","        config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', \n","                        'input_path: \"{}\"'.format(VAL_RECORD_FILE), config)\n","        \n","        # Set test tf-record file path\n","        config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/test)(.*?\")', \n","                        'input_path: \"{}\"'.format(TEST_RECORD_FILE), config)\n","\n","        # Set number of classes. \n","        config = re.sub('num_classes: [0-9]+',\n","                        'num_classes: {}'.format(NUM_CLASSES), config)\n","\n","        # Hyper parameters\n","        ########################################################################\n","\n","        # Set batch size\n","        config = re.sub('batch_size: [0-9]+',\n","                        'batch_size: {}'.format(sweep_config['batch_size']), config)\n","        \n","        # Set training steps\n","        config = re.sub('num_steps: [0-9]+',\n","                        'num_steps: {}'.format(sweep_config['num_steps']), config)\n","            \n","        # CHANGE LEARNING RATE PARAMTERS\n","        # models 'resnet-50-faster' and 'efficientdet-d0' are using \n","        # cosine_decay_learning_rate in momentum optimizer:\n","        if sweep_config['model'] in ['resnet-50-faster', 'efficientdet-d0']:\n","            warmup_learning_rate = sweep_config['learning_rate_base']/100\n","            warmup_steps = sweep_config['num_steps']//25\n","\n","            config = re.sub('learning_rate_base: [-.e0-9]+',\n","                            'learning_rate_base: {}'.format(sweep_config['learning_rate_base']), config)\n","            config = re.sub('warmup_learning_rate: [-.e0-9]+',\n","                            'warmup_learning_rate: {}'.format(warmup_learning_rate), config)\n","            config = re.sub('total_steps: [0-9]+',\n","                            'total_steps: {}'.format(sweep_config['num_steps']), config)\n","            config = re.sub('warmup_steps: [0-9]+',\n","                            'warmup_steps: {}'.format(warmup_steps), config)\n","            \n","        # 'centernet' model is using manual_step_learning_rate:\n","        elif sweep_config['model'] in ['centernet']:\n","            step_update_lr_1 = int(sweep_config['num_steps'] * .6)\n","            step_update_lr_2 = int(sweep_config['num_steps'] * .8)\n","            learning_rate_1 = sweep_config['initial_learning_rate']/10\n","            learning_rate_2 = sweep_config['initial_learning_rate']/100\n","\n","            config = re.sub('initial_learning_rate: [-.e0-9]+',\n","                            'initial_learning_rate: {}'.format(sweep_config['initial_learning_rate']), config)\n","            \n","            config = re.sub('learning_rate_1: [-.e0-9]+',\n","                            'learning_rate: {}'.format(learning_rate_1), config)\n","            config = re.sub('learning_rate_2: [-.e0-9]+',\n","                            'learning_rate: {}'.format(learning_rate_2), config)\n","            \n","            config = re.sub('step_update_lr_1: [-.e0-9]+',\n","                            'step: {}'.format(step_update_lr_1), config)\n","            config = re.sub('step_update_lr_2: [-.e0-9]+',\n","                            'step: {}'.format(step_update_lr_2), config)\n","        ########################################################################\n","        \n","\n","        # Set how may images you want visualized in the evaluation tb-event file\n","        config = re.sub('eval_config: {',\n","                        'eval_config: {' + '\\n  num_visualizations: {}'.format(NUM_VISUALIZATIONS), config)\n","\n","        # Set fine-tune checkpoint type to detection\n","        # more details here: https://drive.google.com/file/d/1diOU07Qfc73R3DvZL9JyYfdMZWhAXCL5/view?usp=sharing\n","        config = re.sub('fine_tune_checkpoint_type: \"classification\"', \n","                        'fine_tune_checkpoint_type: \"{}\"'.format('detection'), config)\n","        f.write(config)\n","\n","        # test if the config is updated:\n","        print(config)\n","\n","\n","def get_model_dir(model):\n","    return '/content/training/' + MODELS_CONFIG[model]['model_name']\n","\n","\n","def get_log_dir(sweep_id):\n","    return '/content/drive/MyDrive/models/research/deploy/logs/' + sweep_id\n","\n","\n","def log_eval_metrics(path):\n","    '''reads a evaluation-tfevent file and returns a list of metrics'''\n","    eval_metrics = []\n","    for summary in summary_iterator(path):\n","        for v in summary.summary.value:\n","            if v.metadata.plugin_data.plugin_name != 'images':\n","                eval_metrics.append({v.tag: tf.make_ndarray(v.tensor).item()})\n","    return eval_metrics\n","\n","\n","def log_train_metrics(path):\n","    '''reads a training-tfevent file and returns a list of train-information'''\n","    train_metrics = []\n","    for summary in summary_iterator(path):\n","        step = summary.step\n","        for v in summary.summary.value:\n","            if v.metadata.plugin_data.plugin_name != 'images':\n","                train_metrics.append({v.tag: tf.make_ndarray(v.tensor).item(), 'Step': step})\n","    return train_metrics"]},{"cell_type":"markdown","metadata":{"id":"f_VRQcovzqgu"},"source":["# Choose Model & Configure Sweep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQkRlPl4ybKo"},"outputs":[],"source":["# if you want to start a new sweep\n","# False if you are connecting to an existing sweep\n","new_sweep = True  \n","# number of runs to execute\n","count = 100\n","# sweep_id if you are connecting to an existing one\n","sweep_id = 'btrorx9e'\n","\n","# We chose to evaluate the three models: 'resnet-50-faster', \n","#                                        'efficientdet-d0', \n","#                                        'centernet'\n","\n","if new_sweep:\n","    # model = 'centernet'\n","\n","    sweep_config = {\n","        'method': 'grid',\n","        'parameters': {\n","            'model': {\n","                'values': ['resnet-50-faster', 'efficientdet-d0']\n","            },\n","            'batch_size': {\n","                'value': 3\n","            },\n","            'num_steps': {\n","                'values': [10000, 20000, 30000]\n","            },\n","            # For efficientdet-d0 and resnet-faster:\n","            'learning_rate_base': {\n","                'values': [0.1, 0.01]\n","            }\n","            # following is only for cosin learning rate (not centernet):\n","            # happens in the update_config_file() function:\n","            # warmup learning rate is calculated to learning_rate/100\n","            # warmup_steps is calculated to num_steps//25\n","\n","            # For Centernet:\n","            # 'initial_learning_rate': {\n","            #     'values': [0.01, 0.03]\n","            # }\n","            # following is only for manual_step_learning_rate (only centernet):\n","            # happens in the update_config_file() function:\n","            # step_update_lr_1 is the lr for the last 40% of steps\n","            #   learning_rate_1: 1/100 of initial_learning_rate\n","            # step_update_lr_2 is the lr for the last 20% of steps \n","            #   learning_rate_2: 1/100 of initial_learning_rate\n","        }\n","    }\n","    #  Initialize new sweep\n","    sweep_id = wandb.sweep(sweep_config, project='model-sweeps', entity='cots-detectors')\n","    print('Created new sweel with sweep_id:{}'.format(sweep_id))\n","\n","\n","\n","# funciton run by wandb sweep agent\n","def sweep_train_eval():\n","    with wandb.init() as run:\n","        config = wandb.config\n","        run_id = run.id\n","\n","        # tfevent files are stored in log_dir\n","        # checkpoint files during training are stored in model_dir\n","        log_dir = get_log_dir(run_id)\n","\n","        wandb.log(dict(config))\n","\n","        # stores tfevent files in log_dir\n","        # and training checkpoints in model_dir\n","        train_eval(dict(config), run_id)\n","\n","        # read metrics from tfevent files and log to wandb\n","        eval_path = os.path.join(log_dir, 'eval')\n","        eval_file = os.path.join(log_dir, 'eval', os.listdir(eval_path)[0])\n","\n","        train_path = os.path.join(log_dir, 'train')\n","        train_file = os.path.join(log_dir, 'train', os.listdir(train_path)[0])\n","\n","        train_metrics = log_train_metrics(train_file)\n","        eval_metrics = log_eval_metrics(eval_file)\n","\n","        for metric in train_metrics:\n","            wandb.log(metric)\n","            \n","        for metric in eval_metrics:\n","            wandb.log(metric)\n","\n","        if STORE_TFEVENT_FILES:\n","            wandb.save(train_file, base_path=log_dir)\n","            wandb.save(eval_file, base_path=log_dir) \n","\n","# connects to sweep\n","wandb.agent(sweep_id, function=sweep_train_eval, count=count)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPomy8gmQmngr+ywSYWY4Hv","collapsed_sections":[],"machine_shape":"hm","name":"local_auto_training.ipynb","private_outputs":true,"provenance":[{"file_id":"1ffwHKtvC8I5epliscuhJZ-elPiOnbFLZ","timestamp":1652454460534}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
